# combined_api.py â€” Whisper + KoBERT + ìœ ì°½ì„± ê³ ë„í™” ì ìš© ë²„ì „
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from transformers import BertForSequenceClassification, BertTokenizer
import torch
import torchaudio
from transformers import BertTokenizer, BertForSequenceClassification
import uuid
import os
import subprocess
import whisper
import re

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ëª¨ë¸ ë¡œë”©
model_path = "./kobert-mvp"
model = BertForSequenceClassification.from_pretrained("gdstorm/kobert-mvp")
tokenizer = BertTokenizer.from_pretrained("gdstorm/kobert-mvp")
model.eval()

whisper_model = whisper.load_model("base")

# ë§ë”ë“¬ ê°ì§€ í•¨ìˆ˜
def count_disfluencies(text):
    stutter_patterns = [r"\b(ì–´)+\b", r"\b(ìŒ)+\b", r"\b(ê·¸)+\b", r"\b(ì—)+\b"]
    count = 0
    for pattern in stutter_patterns:
        matches = re.findall(pattern, text)
        count += len(matches)
    return count

@app.post("/analyze_audio")
async def analyze_audio(
    file: UploadFile = File(...),
    session_id: str = Form(...),
    duration_sec: float = Form(...)
):
    try:
        # 1. ì„ì‹œ ì €ì¥
        temp_filename = f"temp_{uuid.uuid4()}.aac"
        with open(temp_filename, "wb") as f:
            f.write(await file.read())

        # 2. ffmpeg ë³€í™˜
        wav_filename = "temp_converted.wav"
        ffmpeg_command = [
            "ffmpeg", "-y", "-i", temp_filename,
            "-ar", "16000", "-ac", "1", wav_filename
        ]
        subprocess.run(ffmpeg_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        # 3. Whisper ì „ì‚¬
        result = whisper_model.transcribe(wav_filename, language="ko")
        transcription = result["text"].strip()

        if len(transcription) < 50:
            return {"status": "error", "message": "ë°œí™”ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (50ì ì´ìƒ í•„ìš”)."}

        # 4. KoBERT ì˜ˆì¸¡
        inputs = tokenizer(transcription, return_tensors="pt", truncation=True, padding=True, max_length=300)
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            temperature = 1.5
            probs = torch.nn.functional.softmax(logits / temperature, dim=1)
            confidence = torch.max(probs).item()
            predicted = torch.argmax(probs, dim=1).item()

        # 5. ìœ ì°½ì„± ì ìˆ˜ ê³„ì‚°
        words = transcription.strip().split()
        wpm = len(words) / (duration_sec / 60)  # words per minute
        disfluency_penalty = count_disfluencies(transcription) * 1.5  # ê°ì  ìš”ì¸
        fluency_score = max(round(wpm - disfluency_penalty, 1), 0)

        clarity_score = round(confidence * 100, 2)

        # 6. ì½”ë©˜íŠ¸ ì²˜ë¦¬
        if predicted + 3 > 10:
            comment = "ğŸ“› ì˜ˆì¸¡ëœ ì—°ë ¹ì´ ëª¨ë¸ì˜ í•™ìŠµ ë²”ìœ„(3~10ì„¸)ë¥¼ ë²—ì–´ë‚¬ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
        else:
            comment = f"ğŸ§  ì˜ˆì¸¡ëœ ì–¸ì–´ ì—°ë ¹: {predicted+3}ì„¸ (ì‹ ë¢°ë„: {round(confidence * 100, 2)}%)"

        return {
            "status": "evaluated",
            "transcription": transcription,
            "predicted_label": predicted + 3,
            "confidence": round(confidence, 4),
            "fluency_score": fluency_score,
            "clarity_score": clarity_score,
            "comment": comment
        }

    except Exception as e:
        return {"status": "error", "message": str(e)}
    finally:
        for f in [temp_filename, wav_filename]:
            if os.path.exists(f):
                os.remove(f)
